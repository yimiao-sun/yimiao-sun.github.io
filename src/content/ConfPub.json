[
    {
        "bibtex": "@inproceedings{gu2025mass,\ntitle = {MASS: Empowering Wi-Fi Human Sensing with Metasurface-Assisted Sample Synthesis},\nauthor = {Gu, Jiaming and Chen, Shaonan and Sun, Yimiao and Xie, Yadong and Xi, Rui and Cheng, Qiang and He, Yuan},\nbooktitle = {Proceedings of Springer WASA}, \nyear = {2025}\n}\n",
        "authors": {
            "boldAuthors": ["Yimiao Sun"]
        },
        "note": "Springer WASA'25",
        "resources": {
            "pdf": "./files/PDF/2025MASS_WASA.pdf",
            "slides": "./files/Slides/2025MASS_PPT.pdf"
        },
        "abstract": "Wi-Fi human sensing has attracted numerous research studies over the past decade. The rapid advancement of machine learning technology further boosts the development of Wi-Fi human sensing. However, current Wi-Fi human sensing suffers from the \"data scarcity\" problem: all the existing proposals require collecting a large amount of humanbased datasets to train the sensing models, which is labor-intensive and may raise ethical concerns in certain scenarios. This obstacle seriously restricts the size, quality, and diversity of available datasets, thereby affecting the sensing performance in terms of accuracy and cross-domain applicability. In order to solve this problem, we in this paper propose Metasurface-Assisted Sample Synthesis (MASS), a novel approach to synthesize high-fidelity Wi-Fi sensing samples that effectively capture both the essential features of human motion and environment-specific multipath characteristics, without requiring human involvement. The evaluation results show that MASS is effective in augmenting the training set, improving the classification accuracy by 18%, and enhancing the cross-domain sensing accuracy by 22%. These findings underscore the potential of MASS to facilitate the creation of high-quality, diverse datasets with minimal human involvement and associated labor costs."
    },
    {
        "bibtex": "@inproceedings{zou2025satori,\ntitle = {Satori: In-band Analog Backscatter for Audio Transmission},\nauthor = {Zou, Yang and Na, Xin and Sun, Yimiao and Chen, Yande and He, Yuan},\nbooktitle = {Proceedings of ACM MobiSys}, \nyear = {2025}\n}\n",
        "authors": {
            "boldAuthors": ["Yimiao Sun"]
        },
        "note": "ACM MobiSys'25",
        "resources": {
            "pdf": "./files/PDF/2025Satori_MobiSys.pdf",
            "slides": "./files/Slides/2025Satori_PPT.pdf",
            "award": "https://www.sigmobile.org/mobisys/2025/accepted_papers/"
        },
        "abstract": "In IoT applications such as environmental monitoring and industrial security surveillance, audio sensors are increasingly used, among which wireless sensors are preferred. In order to achieve a sustained transmission, low-power wireless technology such as backscatter has been widely considered. However, existing backscatter systems encounter difficulties in audio transmissions due to the high power consumption from the complicated digital processing and fast frequency-shifting clocks. In this paper, we propose Satori, the first-of-its-kind in-band analog backscatter system for audio transmission with ultra-low power consumption. Satori eliminates the need for in-place digital processing by directly embedding analog audio voltages into backscattered WiFi symbols through analog modulation. It also avoids the power consumption of the frequency-shifting clock by transmitting the audio within the excitation WiFi signal's frequency band. We implement the Satori prototype and evaluate it under various settings. The experimental results indicate that Satori achieves audio transmission with a SNR greater than 18 dB. Meanwhile, the ASIC solution consumes only 9.3μW, 3.3× lower than the existing backscatter systems for audio transmission."
    },
    {
        "bibtex": "@inproceedings{na2025quinid,\ntitle = {QuinID: Enabling FDMA-Based Fully Parallel RFID with Frequency-Selective Antenna},\nauthor = {Na, Xin and Zhang, Jia and Zhang, Jiacheng and Guo, Xiuzhen and Zou, Yang and Jin, Meng and Sun, Yimiao and Liu, Yunhao and He, Yuan},\nbooktitle = {Proceedings of ACM MobiCom}, \nyear = {2025}\n}\n",
        "authors": {
            "boldAuthors": ["Yimiao Sun"]
        },
        "note": "ACM MobiCom'25",
        "resources": {
            "pdf": "./files/PDF/2025QuinID_MobiCom.pdf",
            "slides": ""
        },
        "abstract": "Parallelizing passive Radio Frequency Identification (RFID) reading is an arguably crucial, yet unsolved challenge in modern IoT applications. Existing approaches remain limited to time-division operations and fail to read multiple tags simultaneously. In this paper, we introduce QuinID, the first frequency-division multiple access (FDMA) RFID system to achieve fully parallel reading. We innovatively exploit the frequency selectivity of the tag antenna rather than a conventional digital FDMA, bypassing the power and circuitry constraint of RFID tags. Specifically, we delicately design the frequency-selective antenna based on surface acoustic wave (SAW) components to achieve extreme narrow-band response, so that QuinID tags (i.e., QuinTags) operate exclusively within their designated frequency bands. By carefully designing the matching network and canceling various interference, a customized QuinReader communicates simultaneously with multiple QuinTags across distinct bands. QuinID maintains high compatibility with commercial RFID systems and presents a tag cost of less than 10 cents. We implement a 5-band QuinID system and evaluate its performance under various settings. The results demonstrate a fivefold increase in read rate, reaching up to 5000 reads per second."
    },
    {
        "bibtex": "@inproceedings{zhang2023rfinder,\ntitle = {RFinder: Pinpoint the Invisible RFID Tags in the Prefabricated Buildings},\nauthor = {Zhang, Jiacheng and Jin, Meng and Sun, Yimiao and Wang, Weiguo and Zhang, Jia and Na, Xin and Guo, Xiuzhen and He, Yuan},\nbooktitle = {Proceedings of EWSN}, \nyear = {2024}\n}\n",
        "authors": {
            "boldAuthors": ["Yimiao Sun"]
        },
        "note": "EWSN'24",
        "resources": {
            "pdf": "./files/PDF/2024RFinder_EWSN.pdf",
            "slides": "./files/Slides/2024_RFinder_PPT.pdf"
        },
        "abstract": "This paper presents RFinder, a handheld RFID localization system that can accurately localize an in-concrete RFID tag. Such a system facilitates RFID applications in construction quality control. However, the challenge we meet in designing such a system is to achieve accurate RFID localization in a compact (e.g., with only one antenna) and mobile form. To solve this problem, we opportunistically combine the sensing ability of the RFID and the Inertial Measurement Unit (IMU) sensor that is widely embedded in most handheld RFID readers. Specifically, the user just needs to hold the handheld device and move it in the air. The movement of the device causes changes in 1) RFID measurement phase, and 2) motion sensor data (e.g., acceleration) captured by IMU. We find that if the tag’s direction relative to the tag is almost not changed, it is possible to establish a correlation between the velocity calculated from acceleration and that of the RFID phase, thereby enabling us to localize the tag’s AoA without the precise position of the antenna. We build a signal preprocessing and matching workflow to better fuse the two kinds of data. Experiments show that RFinder can achieve an 11° 3D AoA estimation accuracy at the range of 2.5m."
    },
    {
        "bibtex": "@inproceedings{Awais2024mmjaw,\ntitle = {mmJaw: Remote Jaw Gesture Recognition with COTS mmWave Radar},\nauthor = {Siddiqi, Awais Ahmad and He, Yuan and Chen, Yande and Sun, Yimiao and Wang, Shufan and Xie, Yadong},\nbooktitle = {Proceedings of IEEE ICPADS}, \nyear = {2024}\n}\n",
        "authors": {
            "boldAuthors": ["Yimiao Sun"]
        },
        "note": "IEEE ICPADS'24",
        "resources": {
            "pdf": "./files/PDF/2024mmJaw_ICPADS.pdf",
            "slides": "./files/Slides/2024mmJaw_PPT.pdf"
        },
        "abstract": "With the increasing prevalence of IoT devices and smart systems in daily life, there is a growing demand for new modalities in Human-Computer Interaction (HCI) to improve accessibility, particularly for users who require hands-free and eyes-free interaction in contexts like VR environments, as well as for individuals with special needs or limited mobility. In this paper, we propose teeth gestures as an input modality for HCI. We find that teeth gestures, such as tapping, clenching, and sliding, are generated by various facial muscle movements that are often imperceptible to the naked eye but can be effectively captured using mm-wave radar. By capturing and analyzing the distinct patterns of these muscle movements, we propose a hands-free and eyes-free HCI solution based on three different gestures. Key challenges addressed in this paper include user range identification amidst background noise and other irrelevant facial movements. Results from 16 volunteers demonstrate the robustness of our approach, achieving 93% accuracy for up to a 2.5m range."
    },    
    {
        "bibtex": "@inproceedings{mao2024mmhrr,\ntitle = {mmHRR: Monitoring Heart Rate Recovery with Millimeter Wave Radar},\nauthor = {Mao, Ziheng and He, Yuan and Zhang, Jia and Sun, Yimiao and Xie, Yadong and Guo, Xiuzhen},\nbooktitle = {Proceedings of IEEE ICPADS}, \nyear = {2024}\n}\n",
        "authors": {
            "boldAuthors": ["Yimiao Sun"]
        },
        "note": "IEEE ICPADS'24",
        "resources": {
            "pdf": "./files/PDF/2024mmHRR_ICPADS.pdf",
            "slides": "./files/Slides/2024mmHRR_PPT.pdf"
        },
        "abstract": "Heart rate recovery (HRR) within the initial minute following exercise is a widely utilized metric for assessing cardiac autonomic function in individuals and predicting mortality risk in patients with cardiovascular disease. However, prevailing solutions for HRR monitoring typically involve the use of specialized medical equipment or contact wearable sensors, resulting in high costs and poor user experience. In this paper, we propose a contactless HRR monitoring technique, mmHRR, which achieves accurate heart rate (HR) estimation with a commercial mmWave radar. Unlike HR estimation at rest, the HR varies quickly after exercise and the heartbeat signal entangles with the respiration harmonics. To overcome these hurdles and effectively estimate the HR from the weak and non-stationary heartbeat signal, we propose a novel signal processing pipeline, including dynamic target tracking, adaptive heartbeat signal extraction, and accurate HR estimation with composite sliding windows. Real-world experiments demonstrate that mmHRR exhibits exceptional robustness across diverse environmental conditions, and achieves an average HR estimation error of 3.31 bpm (beats per minute), 71% lower than that of the state-of-the-art method."
    },    
    {
        "bibtex": "@inproceedings{chen2024mmtai,\ntitle = {mmTAI: Biometrics-assisted Multi-person Tracking with mmWave Radar},\nauthor = {Chen, Yande and He, Yuan and Sun, Yimiao and Siddiqi, Awais Ahmad and Zhang, Jia and Guo, Xiuzhen},\nbooktitle = {Proceedings of IEEE ICPADS}, \nyear = {2024}\n}\n",
        "authors": {
            "boldAuthors": ["Yimiao Sun"]
        },
        "note": "IEEE ICPADS'24",
        "resources": {
            "pdf": "./files/PDF/2024mmTAI_ICPADS.pdf",
            "slides": "./files/Slides/2024mmTAI_PPT.pdf"
        },
        "abstract": "mmWave-based human tracking is a key enabling technology for smart applications. Most of the existing works on this topic employ the conventional approach of device-free object localization, which treat any person as a general moving target rather than distinguish different persons. As a result, the existing approaches have poor performance in the scenarios of multiperson tracking, especially when there are crossovers among different persons’trajectories. This paper presents mmTAI, a novel approach for multi-person tracking with a mmWave radar. By exploiting mmWave sensing to capture a human’s biometric features, mmTAI augments mmWave radar based human tracking with the ability of identifying different persons. Specifically, mmTAI is able to sense persons’ scalp responses to the signals and their head-shoulder distances, which are then continuously mapped to their trajectories using a bipartite matching algorithm. We implement mmTAI with a commercial mmWave radar and evaluate its performance under various settings. The results show that in the multi-person tracking scenarios, mmTAI has a median tracking error of 12.33 cm, which is 35.88% lower than that of the state-of-the-art approach."
    },
    {
        "bibtex": "@inproceedings{chen2024elase,\ntitle = {ELASE: Enabling Real-time Elastic Sensing Resource Scheduling in 5G vRAN},\nauthor = {Chen, Yulong and Guo, Junchen and Sun, Yimiao and Yao, Haipeng and Liu, Yunhao and He, Yuan},\nbooktitle = {Proceedings of IEEE/ACM IWQoS}, \nyear = {2024}\n}\n",
        "authors": {
            "boldAuthors": ["Yimiao Sun"]
        },
        "note": "IEEE/ACM IWQoS'24",
        "resources": {
            "pdf": "./files/PDF/2024ElaSe_IWQoS.pdf",
            "slides": "./files/Slides/2024ElaSe_PPT.pdf"
        },
        "abstract": "Integrated Sensing and Communication (ISAC) has been witnessed to be a new paradigm of wireless sensing in 5G networks. Users can benefit from pervasive sensing applications in various scenarios with no communication penalty. Given the diverse demands for sensing resources across different sensing tasks, elastic resource scheduling becomes crucial, particularly when resources are constrained. However, existing approaches often treat users equally, limiting their applicability in dealing with diverse sensing tasks in the real world. In this paper, we introduce ElaSe, a pioneering sensing technique that enables real-time elastic scheduling of sensing resources. At the core of ElaSe is the exploration of the user’s state to precisely determine the sensing resource requirements and schedule resources accordingly. We build the first model for matching sensing resources with sensing demands, and further propose a predictive scheduling scheme to eliminate delays by leveraging the 5G virtualized radio access network (vRAN). We conduct experiments to evaluate the performance of ElaSe under different settings. The results demonstrate that ElaSe outperforms the non-scheduling scheme, with a 34% reduction in trajectory tracking error and a 92% decrease in resource allocation error."
    },
    {
        "bibtex": "@inproceedings{zou2024trident,\ntitle = {TRIDENT: Interference Avoidance in Multi-reader Backscatter Network via Frequency-space Division},\nauthor = {Zou, Yang and Na, Xin and Guo, Xiuzhen and Sun, Yimiao and He, Yuan},\nbooktitle = {Proceedings of IEEE INFOCOM}, \nyear = {2024}\n}\n",
        "authors": {
            "boldAuthors": ["Yimiao Sun"]
        },
        "note": "IEEE INFOCOM'24",
        "resources": {
            "pdf": "./files/PDF/2024TRIDENT_INFOCOM.pdf",
            "slides": "./files/Slides/2024TRIDENT_PPT.pdf"
        },
        "abstract": "Backscatter is an enabling technology for battery-free sensing in industrial IoT applications. For the purpose of full coverage of numerous tags in the deployment area, one often needs to deploy multiple readers, each of which is to communicate with tags within its communication range. But the actual backscattered signals from a tag are likely to reach a reader outside its communication range, causing undesired interference. Conventional approaches for interference avoidance, either TDMA or CSMA based, separate the readers’ media accesses in the time dimension and suffer from limited network throughput. In this paper, we propose TRIDENT, a novel backscatter tag design that enables interference avoidance with frequency-space division. By incorporating a tunable bandpass filter and multiple terminal loads, a TRIDENT tag is able to detect its channel condition and adaptively adjust the frequency band and the power of its backscattered signals, so that all the readers in the network can operate concurrently without being interfered. We implement TRIDENT and evaluate its performance under various settings. The results demonstrate that TRIDENT enhances the network throughput by 3.18×, compared to the TDMA based scheme."
    },
    {
        "bibtex": "@inproceedings{sun2023bifrost,\ntitle = {BIFROST: Reinventing WiFi Signals Based on Dispersion Effect for Accurate Indoor Localization},\nauthor = {Sun, Yimiao and He, Yuan and Zhang, Jiacheng and Na, Xin and Chen, Yande and Wang, Weiguo and Guo, Xiuzhen},\nbooktitle = {Proceedings of ACM SenSys}, \nyear = {2023}\n}\n",
        "authors": {
            "boldAuthors": ["Yimiao Sun"]
        },
        "note": "ACM SenSys'23",
        "resources": {
            "pdf": "./files/PDF/2023BIFROST_SenSys.pdf",
            "slides": "./files/Slides/2023BIFROST_PPT.pdf"
        },
        "abstract": "WiFi-based device localization is a key enabling technology for smart applications, which has attracted numerous research studies in the past decade. Most of the existing approaches rely on Line-of-Sight (LoS) signals to work, while a critical problem is often neglected: In the real-world indoor environments, WiFi signals are everywhere, but very few of them are usable for accurate localization. As a result, the localization accuracy in practice is far from being satisfactory. This paper presents BIFROST, a novel hardware-software co-design for accurate indoor localization. The core idea of BIFROST is to reinvent WiFi signals, so as to provide sufficient LoS signals for localization. This is realized by exploiting the dispersion effect of signals emitted by the leaky wave antenna (LWA). We present a low-cost plug-in design of LWA that can generate orthogonal polarized signals: On one hand, LWA disperses signals of different frequencies to different angles, thus providing Angle-of-Arrival (AoA) information for the localized target. On the other hand, the target further leverages the antenna polarization mismatch to distinguish AoAs from different LWAs. In the software layer, fine-grained information in Channel State Information (CSI) is exploited to cope with multipath and noise. We implement BIFROST and evaluate its performance under various settings. The results show that the median localization error of BIFROST is 0.81m, which is 52.35% less than that of SpotFi, a state-of-the-art approach. SpotFi, when combined with BIFROST to work in the realistic settings, can reduce the localization error by 33.54%."
    },
    {
        "bibtex": "@inproceedings{wang2023metaspeaker,\ntitle = {Meta-Speaker: Acoustic Source Projection by Exploiting Air Nonlinearity},\nauthor = {Wang, Weiguo and He, Yuan and Jin, Meng and Sun, Yimiao and Guo, Xiuzhen},\nbooktitle = {Proceedings of ACM MobiCom}, \nyear = {2023}\n}\n",
        "authors": {
            "boldAuthors": ["Yimiao Sun"]
        },
        "note": "ACM MobiCom'23",
        "resources": {
            "pdf": "./files/PDF/2023MetaSpeaker_MobiCom.pdf",
            "slides": "./files/Slides/2023MetaSpeaker_PPT.pdf"
        },
        "abstract": "This paper proposes Meta-Speaker, an innovative speaker capable of projecting audible sources into the air with a high level of manipulability. Unlike traditional speakers that emit sound waves in all directions, Meta-Speaker can manipulate the granularity of the audible region, down to a single point, and can manipulate the location of the source. Additionally, the source projected by Meta-Speaker is a physical presence in space, allowing both humans and machines to perceive it with spatial awareness. Meta-Speaker achieves this by leveraging the fact that air is a nonlinear medium, which enables the reproduction of audible sources from ultrasounds. Meta-Speaker comprises two distributed ultrasonic arrays, each transmitting a narrow ultrasonic beam. The audible source can be reproduced at the intersection of the beams. We present a comprehensive profiling of Meta-Speaker to validate the high manipulability it offers. We prototype Meta-Speaker and demonstrate its potential through three applications: anchor-free localization with a median error of 0.13 m, location-aware communication with a throughput of 1.28 Kbps, and acoustic augmented reality where users can perceive source direction with a mean error of 9.8 degrees."
    },
    {
        "bibtex": "@inproceedings{zhang2023mmhawkeve,\ntitle = {mmHawkeye: Passive UAV Detection with a COTS mmWave Radar},\nauthor = {Zhang, Jia and Na, Xin and Xi, Rui and Sun, Yimiao and He, Yuan},\nbooktitle = {Proceedings of IEEE SECON}, \nyear = {2023}\n}\n",
        "authors": {
            "boldAuthors": ["Yimiao Sun"]
        },
        "note": "IEEE SECON'23",
        "resources": {
            "pdf": "./files/PDF/2023mmHawkeye_SECON.pdf",
            "slides": "./files/Slides/2023mmHawkeye_PPT.pdf"
        },
        "abstract": "Small Unmanned Aerial Vehicles (UAVs) are becoming potential threats to security-sensitive areas and personal  privacy.  A UAV can shoot photos at height, but how to detect such  an uninvited intruder is an open problem.  This paper presents  mmHawkeye, a passive approach for UAV detection with a COTS  millimeter wave (mmWave) radar.  mmHawkeye doesn’t require  prior knowledge of the type, motions, and flight trajectory of  the UAV, while exploiting the signal feature induced by the UAV’s periodic micro-motion (PMM) for long-range accurate  detection.  The design is therefore effective in dealing with lowSNR and uncertain reflected signals from the UAV.  mmHawkeye  can further track the UAV’s position with dynamic programming  and particle filtering, and identify it with a Long Short-Term Memory (LSTM) based detector.  We implement mmHawkeye on  a commercial mmWave radar and evaluate its performance under  varied settings.  The experimental results show that mmHawkeye  has a detection accuracy of 95.8% and can realize detection at  a range up to 80m."
    },    
    {
        "bibtex": "@inproceedings{sun2023aim,\ntitle = {AIM: Acoustic Inertial Measurement for Indoor Drone Localization and Tracking},\nauthor = {Sun, Yimiao and Wang, Weiguo and Mottola, Luca and Wang, Ruijin and He, Yuan},\nbooktitle = {Proceedings of ACM SenSys},\nyear = {2022}\n}\n",
        "authors": {
            "boldAuthors": ["Yimiao Sun"]
        },
        "note": "ACM SenSys'22",
        "resources": {
            "pdf": "./files/PDF/2022AIM_SenSys.pdf",
            "slides": "./files/Slides/2022AIM_PPT.pdf",
            "code": null,
            "demo": null
        },
        "abstract": "We present Acoustic Inertial Measurement (AIM), a one-of-a-kind technique for indoor drone localization and tracking. Indoor drone localization and tracking are arguably a crucial, yet unsolved challenge: in GPS-denied environments, existing approaches enjoy limited applicability, especially in Non-Line of Sight (NLoS), require extensive environment instrumentation, or demand considerable hardware/software changes on drones. In contrast, AIM exploits the acoustic characteristics of the drones to estimate their location and derive their motion, even in NLoS settings. We tame location estimation errors using a dedicated Kalman filter and the Interquartile Range rule (IQR). We implement AIM using an off-the-shelf microphone array and evaluate its performance with a commercial drone under varied settings. Results indicate that the mean localization error of AIM is 46% lower than commercial UWB-based systems in complex indoor scenarios, where state-of-the-art infrared systems would not even work because of NLoS settings. We further demonstrate that AIM can be extended to support indoor spaces with arbitrary ranges and layouts without loss of accuracy by deploying distributed microphone arrays."
    },
    {
        "bibtex": "@inproceedings{wang2023micnest,\ntitle = {MicNest: Long-Range Instant Acoustic Localization of Drones in Precise Landing},\nauthor = {Wang, Weiguo and Mottola, Luca and He, Yuan and Li, Jinming and Sun, Yimiao and Li, Shuai and Jing, Hua and Wang, Yulei},\nbooktitle = {Proceedings of ACM SenSys},\nyear = {2022}\n}\n",
        "authors": {
            "boldAuthors": ["Yimiao Sun"]
        },
        "note": "ACM SenSys'22",
        "resources": {
            "pdf": "./files/PDF/2022MicNest_SenSys.pdf",
            "slides": "./files/Slides/2022MicNest_PPT.pdf",
            "code": null,
            "home": "https://micnest.github.io/",
            "award": "https://sensys.acm.org/2022/award/"
        },
        "abstract": "We present MicNest: an acoustic localization system enabling precise landing of aerial drones. Drone landing is a crucial step in a drone's operation, especially as high-bandwidth wireless networks, such as 5G, enable beyond-line-of-sight operation in a shared airspace and applications such as instant asset delivery with drones gain traction. In MicNest, multiple microphones are deployed on a landing platform in carefully devised configurations. The drone carries a speaker transmitting purposefully-designed acoustic pulses. The drone may be localized as long as the pulses are correctly detected. Doing so is challenging: i) because of limited transmission power, propagation attenuation, background noise, and propeller interference, the Signal-to-Noise Ratio (SNR) of received pulses is intrinsically low; ii) the pulses experience non-linear Doppler distortion due to the physical drone dynamics while airborne; iii) as location information is to be used during landing, the processing latency must be reduced to effectively feed the flight control loop. To tackle these issues, we design a novel pulse detector, Matched Filter Tree (MFT), whose idea is to convert pulse detection to a tree search problem. We further present three practical methods to accelerate tree search jointly. Our real-world experiments show that MicNest is able to localize a drone 120 m away with 0.53% relative localization error at 20 Hz location update frequency."
    }

]